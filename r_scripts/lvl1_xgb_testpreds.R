


# lvl1_xgb_testpreds

# can we experiment with predicting on the models generated by level one so far (get a feel for public leaderboard)
# 2 things we're interested in here:
#    1)   loading in the cross-referenced train predictions for generating level 2 predictions
#    1.5) will also want to determine local accuracy for stacked predictions from level 1
#    2)   loading in pre-trained models and generating test predictions to test PLB for level 1 so far


source("r_scripts/lvl1_xgb_config.R")
test <- readRDS("input/test.rds")
samp <- read.csv("input/sample_submission.csv", stringsAsFactors = F)
lvl1_results <- read.csv("r_scripts/level1_results.csv", stringsAsFactors = F)

    # lvl1_res_auc <- lvl1_results[grepl("=auc", lvl1_results$paramkey), ]
    # lvl1_res_logloss <- lvl1_results[grepl("=logloss", lvl1_results$paramkey), ]
    lvl1_res_error <- lvl1_results[grepl("=error", lvl1_results$paramkey), ]
    # 
    # 
    # lvl1_res_auc$iteration[which.max(lvl1_res_auc$cv_score)]           # iteration #31 is the best in auc
    # lvl1_res_logloss$iteration[which.min(lvl1_res_logloss$cv_score)]   # iteration #34 is the best in logloss
    # lvl1_res_error$iteration[which.min(lvl1_res_error$cv_score)]       # iteration #1 is the best in error





# test a single model's pipeline:
files_mods <- list.files(fp_dir_models)
files_preds <- list.files(fp_dir_preds)
files_feats <- list.files(fp_dir_feats)
num_pred_sets <- nrow(lvl1_results)



# loading and process training preds -------------------------------------------------------------
load("cache/level1_files.RData")
rm(train, trainA, trainB, Y) # I just want idA, idB, YA, YB
gc()

files_preds








# generating level1 test predictions --------------------------------------------------------------


# initialize a zero'd-out matrix to gather predictions
test_pred_mat <- matrix(rep(rep(0, nrow(test)), num_pred_sets), nrow=nrow(test))
for(i in 1:num_pred_sets) {
    
    
        # # single model isolation
        # best auc so far -- 31
        # i <- 1
    
    
    # read in feats used for this model and subset test by those
    feats_ <- readRDS(file.path(fp_dir_feats, files_feats[i]))
    test_dmat_ <- xgb.DMatrix(as.matrix(test[, feats_]))
    
    # read in model and pass test features into it for predictions
    xgb_mod_ <- readRDS(file.path(fp_dir_models, files_mods[i]))
    test_preds_ <- predict(xgb_mod_, test_dmat_)
    test_pred_mat[, i] <- test_preds_
        
    
        # # single model sub:
        # single_sub <- data.frame(id=test$id, target=test_preds_)
        # write.csv(single_sub, "subs/03_best_error_after_35_iters.csv", row.names = F)
            
}


# bag all (mean) the values across rows (different models) to get mean score
test_pred_mat_bagged <- apply(test_pred_mat, 1, mean)
bagged_sub <- data.frame(id=test$id, target=test_pred_mat_bagged)
write.csv(bagged_sub, "subs/05_bagged_sub_all_after_94_iters.csv", row.names=F)



# correlation matrix
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
test_pred_cor <- cor(test_pred_mat)
corrplot(test_pred_cor, method=c("ellipse"), type="lower")
corrplot(test_pred_cor, method=c("number"), type="lower")

hist(test_pred_mat[, 5], col='light blue')




# submission format
sub <- data.frame(id=samp$id, target=test_pred_mat_bagged)


# can also feed these into a second model 


